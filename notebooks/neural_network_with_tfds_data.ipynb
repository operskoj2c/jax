{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18AF5Ab4p6VL"
   },
   "source": [
    "##### Copyright 2018 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crfqaJOyp8bq"
   },
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_XlLLpcWjkA"
   },
   "source": [
    "# Training a Simple Neural Network, with tensorflow/datasets Data Loading\n",
    "\n",
    "_Forked from_ `neural_network_and_data_loading.ipynb`\n",
    "\n",
    "_Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary_\n",
    "\n",
    "![JAX](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n",
    "\n",
    "Let's combine everything we showed in the [quickstart notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb) to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use `tensorflow/datasets` data loading API to load images and labels (because it's pretty great, and the world doesn't need yet another data loading library :P).\n",
    "\n",
    "Of course, you can use JAX with any API that is compatible with NumPy to make specifying the model a bit more plug-and-play. Here, just for explanatory purposes, we won't use any neural network libraries or special APIs for builidng our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8OFzj9TqXof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n",
    "!pip install --upgrade jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OksHydJDtbbI"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTVcKi-ZYB3R"
   },
   "source": [
    "### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fmWA06xYE7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/rsepassi/python/jax/local/lib/python2.7/site-packages/jax/lib/xla_bridge.py:146: UserWarning: No GPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "param_scale = 0.1\n",
    "step_size = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtoNk_yxWtIw"
   },
   "source": [
    "### Auto-batching predictions\n",
    "\n",
    "Let us first define our prediction function. Note that we're defining this for a _single_ image example. We're going to use JAX's `vmap` function to automatically handle mini-batches, with no performance penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7APc6tD7TiuZ"
   },
   "outputs": [],
   "source": [
    "from jax.scipy.misc import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(final_w, activations) + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRW_TvCTWgaP"
   },
   "source": [
    "Let's check that our prediction function only works on single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sW2A5mnXHc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpyQxuedXfhp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid shapes!\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work with a batch\n",
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
    "try:\n",
    "  preds = predict(params, random_flattened_images)\n",
    "except TypeError:\n",
    "  print('Invalid shapes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJOOncKMXbwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's upgrade it to handle batches using `vmap`\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "# `batched_predict` has the same call signature as `predict`\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elsG6nX03BvW"
   },
   "source": [
    "At this point, we have all the ingredients we need to define our neural network and train it. We've built an auto-batched version of `predict`, which we should be able to use in a loss function. We should be able to use `grad` to take the derivative of the loss with respect to the neural network parameters. Last, we should be able to use `jit` to speed up everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwDuFqc9X7ER"
   },
   "source": [
    "### Utility and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lTI6I4lWdh5"
   },
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=np.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return np.array(x[:, None] == np.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(batched_predict(params, images), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  preds = batched_predict(params, images)\n",
    "  return -np.sum(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umJJGZCC2oKl"
   },
   "source": [
    "### Data Loading with `tensorflow/datasets`\n",
    "\n",
    "JAX is laser-focused on program transformations and accelerator-backed NumPy, so we don't include data loading or munging in the JAX library. There are already a lot of great data loaders out there, so let's just use them instead of reinventing anything. We'll use the `tensorflow/datasets` data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEvWt8_u2pqG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install tensorflow-datasets\n",
    "# TODO(rsepassi): Switch to stable version on release\n",
    "!pip install -q --upgrade tfds-nightly tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading / extracting dataset mnist (11.06 MiB) to /tmp/tfds/mnist/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.28 url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.28 url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.28 url/s]]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/1 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 2/2 [00:00<00:00,  3.02 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]\n",
      "Dl Size...:  10%|█         | 1/10 [00:00<00:06,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.28 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  20%|██        | 2/10 [00:00<00:06,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  20%|██        | 2/10 [00:00<00:06,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  20%|██        | 2/10 [00:00<00:06,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|███       | 3/10 [00:00<00:05,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  67%|██████▋   | 2/3 [00:00<00:00,  3.02 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|███       | 3/10 [00:00<00:05,  1.29 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:00<00:00,  3.13 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]\n",
      "Dl Size...:  40%|████      | 4/10 [00:00<00:03,  1.79 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  50%|█████     | 5/10 [00:00<00:02,  1.79 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  60%|██████    | 6/10 [00:00<00:02,  1.79 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  70%|███████   | 7/10 [00:01<00:01,  1.79 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:01<00:00,  3.13 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  3.45 url/s]\n",
      "Dl Size...:  80%|████████  | 8/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  90%|█████████ | 9/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  3.45 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  3.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  3.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|███████▌  | 3/4 [00:01<00:00,  3.13 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  3.38 url/s]2 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  2.50 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:01<00:00,  2.52 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  6.44 MiB/s]\u001b[A\n",
      "6 examples [00:00, 59.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000 examples [00:42, 1422.72 examples/s]\n",
      "Shuffling...:   0%|          | 0/10 [00:00<?, ? shard/s]WARNING: Logging before flag parsing goes to stderr.\n",
      "W0129 23:00:21.376614 139949807531776 deprecation.py:323] From /usr/local/google/home/rsepassi/python/jax/local/lib/python2.7/site-packages/tensorflow_datasets/core/file_format_adapter.py:249: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 292112.96 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 268699.14 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 353919.84 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  20%|██        | 2/10 [00:00<00:00, 17.18 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 324690.98 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 269747.51 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 331652.93 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  40%|████      | 4/10 [00:00<00:00, 16.87 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 415421.58 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 271274.07 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 352215.87 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  60%|██████    | 6/10 [00:00<00:00, 17.03 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 350123.46 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 202948.56 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 428084.85 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  80%|████████  | 8/10 [00:00<00:00, 17.12 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 363436.89 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 247402.91 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 399330.75 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...: 100%|██████████| 10/10 [00:00<00:00, 17.41 shard/s]amples/s]\u001b[A\n",
      "10000 examples [00:07, 1410.65 examples/s]\n",
      "Shuffling...:   0%|          | 0/1 [00:00<?, ? shard/s]\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 10000 examples [00:00, 364497.05 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...: 100%|██████████| 1/1 [00:00<00:00,  8.87 shard/s]0 examples/s]\u001b[A\n",
      "Computing statistics...:   0%|          | 0/2 [00:00<?, ? split/s]W0129 23:00:30.799828 139949807531776 deprecation.py:323] From /usr/local/google/home/rsepassi/python/jax/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1730: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "\n",
      "0 examples [00:00, ? examples/s]\u001b[A\n",
      "58 examples [00:00, 579.70 examples/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "301 examples [00:00, 751.16 examples/s]\u001b[A\n",
      "563 examples [00:00, 955.27 examples/s]\u001b[A\n",
      "821 examples [00:00, 1177.28 examples/s]\u001b[A\n",
      "1058 examples [00:00, 1386.31 examples/s]\u001b[A\n",
      "1324 examples [00:00, 1618.32 examples/s]\u001b[A\n",
      "1566 examples [00:00, 1796.02 examples/s]\u001b[A\n",
      "1853 examples [00:00, 2021.03 examples/s]\u001b[A\n",
      "2147 examples [00:00, 2228.51 examples/s]\u001b[A\n",
      "2404 examples [00:01, 2306.40 examples/s]\u001b[A\n",
      "2660 examples [00:01, 2345.34 examples/s]\u001b[A\n",
      "2929 examples [00:01, 2437.74 examples/s]\u001b[A\n",
      "3200 examples [00:01, 2512.12 examples/s]\u001b[A\n",
      "3461 examples [00:01, 2517.43 examples/s]\u001b[A\n",
      "3728 examples [00:01, 2560.58 examples/s]\u001b[A\n",
      "4022 examples [00:01, 2662.09 examples/s]\u001b[A\n",
      "4293 examples [00:01, 2671.85 examples/s]\u001b[A\n",
      "4564 examples [00:01, 2577.09 examples/s]\u001b[A\n",
      "4825 examples [00:01, 2550.97 examples/s]\u001b[A\n",
      "5098 examples [00:02, 2600.71 examples/s]\u001b[A\n",
      "5360 examples [00:02, 2573.46 examples/s]\u001b[A\n",
      "5648 examples [00:02, 2658.01 examples/s]\u001b[A\n",
      "5916 examples [00:02, 2657.79 examples/s]\u001b[A\n",
      "6183 examples [00:02, 2585.20 examples/s]\u001b[A\n",
      "6447 examples [00:02, 2601.08 examples/s]\u001b[A\n",
      "6708 examples [00:02, 2577.84 examples/s]\u001b[A\n",
      "6971 examples [00:02, 2592.59 examples/s]\u001b[A\n",
      "7231 examples [00:02, 2529.44 examples/s]\u001b[A\n",
      "7487 examples [00:02, 2537.16 examples/s]\u001b[A\n",
      "7742 examples [00:03, 2506.07 examples/s]\u001b[A\n",
      "7994 examples [00:03, 2475.96 examples/s]\u001b[A\n",
      "8248 examples [00:03, 2491.43 examples/s]\u001b[A\n",
      "8498 examples [00:03, 2485.43 examples/s]\u001b[A\n",
      "8755 examples [00:03, 2509.06 examples/s]\u001b[A\n",
      "9067 examples [00:03, 2663.85 examples/s]\u001b[A\n",
      "9380 examples [00:03, 2787.68 examples/s]\u001b[A\n",
      "9663 examples [00:03, 2792.48 examples/s]\u001b[A\n",
      "9945 examples [00:03, 2719.39 examples/s]\u001b[A\n",
      "Computing statistics...:  50%|█████     | 1/2 [00:05<00:05,  5.54s/ split]\n",
      "0 examples [00:00, ? examples/s]\u001b[A\n",
      "66 examples [00:00, 658.83 examples/s]\u001b[A\n",
      "316 examples [00:00, 845.59 examples/s]\u001b[A\n",
      "569 examples [00:00, 1056.02 examples/s]\u001b[A\n",
      "821 examples [00:00, 1278.52 examples/s]\u001b[A\n",
      "1074 examples [00:00, 1499.85 examples/s]\u001b[A\n",
      "1303 examples [00:00, 1671.51 examples/s]\u001b[A\n",
      "1547 examples [00:00, 1843.44 examples/s]\u001b[A\n",
      "1800 examples [00:00, 2004.58 examples/s]\u001b[A\n",
      "2042 examples [00:00, 2112.00 examples/s]\u001b[A\n",
      "2304 examples [00:01, 2239.25 examples/s]\u001b[A\n",
      "2545 examples [00:01, 2287.27 examples/s]\u001b[A\n",
      "2786 examples [00:01, 2239.54 examples/s]\u001b[A\n",
      "3019 examples [00:01, 2228.14 examples/s]\u001b[A\n",
      "3248 examples [00:01, 2217.75 examples/s]\u001b[A\n",
      "3528 examples [00:01, 2365.11 examples/s]\u001b[A\n",
      "3772 examples [00:01, 2383.36 examples/s]\u001b[A\n",
      "4015 examples [00:01, 2352.46 examples/s]\u001b[A\n",
      "4272 examples [00:01, 2413.30 examples/s]\u001b[A\n",
      "4516 examples [00:01, 2355.70 examples/s]\u001b[A\n",
      "4797 examples [00:02, 2475.56 examples/s]\u001b[A\n",
      "5067 examples [00:02, 2538.36 examples/s]\u001b[A\n",
      "5333 examples [00:02, 2571.21 examples/s]\u001b[A\n",
      "5609 examples [00:02, 2622.93 examples/s]\u001b[A\n",
      "5877 examples [00:02, 2637.62 examples/s]\u001b[A\n",
      "6152 examples [00:02, 2669.20 examples/s]\u001b[A\n",
      "6430 examples [00:02, 2699.26 examples/s]\u001b[A\n",
      "6701 examples [00:02, 2695.30 examples/s]\u001b[A\n",
      "6972 examples [00:02, 2533.05 examples/s]\u001b[A\n",
      "7228 examples [00:02, 2426.81 examples/s]\u001b[A\n",
      "7474 examples [00:03, 2390.84 examples/s]\u001b[A\n",
      "7721 examples [00:03, 2412.28 examples/s]\u001b[A\n",
      "7964 examples [00:03, 2368.10 examples/s]\u001b[A\n",
      "8209 examples [00:03, 2391.67 examples/s]\u001b[A\n",
      "8450 examples [00:03, 2364.82 examples/s]\u001b[A\n",
      "8688 examples [00:03, 2317.31 examples/s]\u001b[A\n",
      "8929 examples [00:03, 2341.60 examples/s]\u001b[A\n",
      "9164 examples [00:03, 2330.12 examples/s]\u001b[A\n",
      "9398 examples [00:03, 2319.80 examples/s]\u001b[A\n",
      "9631 examples [00:04, 2316.86 examples/s]\u001b[A\n",
      "9888 examples [00:04, 2385.67 examples/s]\u001b[A\n",
      "10128 examples [00:04, 2361.50 examples/s]\u001b[A\n",
      "10417 examples [00:04, 2497.99 examples/s]\u001b[A\n",
      "10715 examples [00:04, 2625.06 examples/s]\u001b[A\n",
      "11012 examples [00:04, 2717.46 examples/s]\u001b[A\n",
      "11314 examples [00:04, 2799.85 examples/s]\u001b[A\n",
      "11597 examples [00:04, 2762.32 examples/s]\u001b[A\n",
      "11876 examples [00:04, 2750.67 examples/s]\u001b[A\n",
      "12153 examples [00:04, 2634.39 examples/s]\u001b[A\n",
      "12419 examples [00:05, 2594.17 examples/s]\u001b[A\n",
      "12690 examples [00:05, 2625.48 examples/s]\u001b[A\n",
      "12954 examples [00:05, 2618.79 examples/s]\u001b[A\n",
      "13217 examples [00:05, 2598.18 examples/s]\u001b[A\n",
      "13481 examples [00:05, 2608.28 examples/s]\u001b[A\n",
      "13743 examples [00:05, 2532.07 examples/s]\u001b[A\n",
      "13998 examples [00:05, 2516.74 examples/s]\u001b[A\n",
      "14251 examples [00:05, 2503.37 examples/s]\u001b[A\n",
      "14502 examples [00:05, 2465.16 examples/s]\u001b[A\n",
      "14749 examples [00:05, 2454.57 examples/s]\u001b[A\n",
      "14996 examples [00:06, 2456.66 examples/s]\u001b[A\n",
      "15242 examples [00:06, 2415.25 examples/s]\u001b[A\n",
      "15552 examples [00:06, 2585.75 examples/s]\u001b[A\n",
      "15815 examples [00:06, 2559.93 examples/s]\u001b[A\n",
      "16077 examples [00:06, 2576.79 examples/s]\u001b[A\n",
      "16361 examples [00:06, 2647.32 examples/s]\u001b[A\n",
      "16628 examples [00:06, 2617.40 examples/s]\u001b[A\n",
      "16892 examples [00:06, 2586.43 examples/s]\u001b[A\n",
      "17152 examples [00:06, 2541.78 examples/s]\u001b[A\n",
      "17429 examples [00:07, 2603.98 examples/s]\u001b[A\n",
      "17691 examples [00:07, 2571.82 examples/s]\u001b[A\n",
      "17955 examples [00:07, 2588.63 examples/s]\u001b[A\n",
      "18215 examples [00:07, 2554.82 examples/s]\u001b[A\n",
      "18471 examples [00:07, 2519.44 examples/s]\u001b[A\n",
      "18724 examples [00:07, 2504.81 examples/s]\u001b[A\n",
      "18975 examples [00:07, 2487.92 examples/s]\u001b[A\n",
      "19225 examples [00:07, 2402.43 examples/s]\u001b[A\n",
      "19467 examples [00:07, 2390.79 examples/s]\u001b[A\n",
      "19710 examples [00:07, 2398.92 examples/s]\u001b[A\n",
      "19977 examples [00:08, 2472.81 examples/s]\u001b[A\n",
      "20238 examples [00:08, 2509.75 examples/s]\u001b[A\n",
      "20535 examples [00:08, 2631.82 examples/s]\u001b[A\n",
      "20808 examples [00:08, 2658.70 examples/s]\u001b[A\n",
      "21116 examples [00:08, 2769.91 examples/s]\u001b[A\n",
      "21410 examples [00:08, 2817.97 examples/s]\u001b[A\n",
      "21694 examples [00:08, 2808.81 examples/s]\u001b[A\n",
      "22006 examples [00:08, 2893.62 examples/s]\u001b[A\n",
      "22297 examples [00:08, 2823.85 examples/s]\u001b[A\n",
      "22581 examples [00:08, 2798.66 examples/s]\u001b[A\n",
      "22862 examples [00:09, 2708.16 examples/s]\u001b[A\n",
      "23136 examples [00:09, 2715.33 examples/s]\u001b[A\n",
      "23422 examples [00:09, 2753.93 examples/s]\u001b[A\n",
      "23721 examples [00:09, 2818.37 examples/s]\u001b[A\n",
      "24066 examples [00:09, 2979.29 examples/s]\u001b[A\n",
      "24368 examples [00:09, 2890.20 examples/s]\u001b[A\n",
      "24686 examples [00:09, 2970.98 examples/s]\u001b[A\n",
      "24987 examples [00:09, 2974.92 examples/s]\u001b[A\n",
      "25294 examples [00:09, 3002.73 examples/s]\u001b[A\n",
      "25596 examples [00:09, 3000.64 examples/s]\u001b[A\n",
      "25897 examples [00:10, 2991.52 examples/s]\u001b[A\n",
      "26197 examples [00:10, 2990.90 examples/s]\u001b[A\n",
      "26501 examples [00:10, 3002.87 examples/s]\u001b[A\n",
      "26802 examples [00:10, 2836.40 examples/s]\u001b[A\n",
      "27088 examples [00:10, 2759.47 examples/s]\u001b[A\n",
      "27366 examples [00:10, 2637.75 examples/s]\u001b[A\n",
      "27641 examples [00:10, 2669.35 examples/s]\u001b[A\n",
      "27910 examples [00:10, 2586.42 examples/s]\u001b[A\n",
      "28171 examples [00:10, 2530.95 examples/s]\u001b[A\n",
      "28426 examples [00:11, 2473.39 examples/s]\u001b[A\n",
      "28678 examples [00:11, 2485.96 examples/s]\u001b[A\n",
      "28935 examples [00:11, 2508.61 examples/s]\u001b[A\n",
      "29187 examples [00:11, 2502.39 examples/s]\u001b[A\n",
      "29446 examples [00:11, 2527.71 examples/s]\u001b[A\n",
      "29725 examples [00:11, 2599.32 examples/s]\u001b[A\n",
      "29996 examples [00:11, 2628.35 examples/s]\u001b[A\n",
      "30323 examples [00:11, 2790.54 examples/s]\u001b[A\n",
      "30629 examples [00:11, 2864.41 examples/s]\u001b[A\n",
      "30920 examples [00:11, 2875.65 examples/s]\u001b[A\n",
      "31215 examples [00:12, 2897.41 examples/s]\u001b[A\n",
      "31507 examples [00:12, 2835.61 examples/s]\u001b[A\n",
      "31823 examples [00:12, 2924.15 examples/s]\u001b[A\n",
      "32117 examples [00:12, 2913.06 examples/s]\u001b[A\n",
      "32415 examples [00:12, 2930.80 examples/s]\u001b[A\n",
      "32709 examples [00:12, 2886.29 examples/s]\u001b[A\n",
      "33009 examples [00:12, 2916.77 examples/s]\u001b[A\n",
      "33316 examples [00:12, 2960.07 examples/s]\u001b[A\n",
      "33637 examples [00:12, 3025.72 examples/s]\u001b[A\n",
      "33941 examples [00:12, 2936.56 examples/s]\u001b[A\n",
      "34236 examples [00:13, 2807.81 examples/s]\u001b[A\n",
      "34519 examples [00:13, 2788.60 examples/s]\u001b[A\n",
      "34826 examples [00:13, 2863.80 examples/s]\u001b[A\n",
      "35114 examples [00:13, 2779.05 examples/s]\u001b[A\n",
      "35419 examples [00:13, 2853.18 examples/s]\u001b[A\n",
      "35706 examples [00:13, 2809.19 examples/s]\u001b[A\n",
      "35989 examples [00:13, 2812.04 examples/s]\u001b[A\n",
      "36290 examples [00:13, 2867.40 examples/s]\u001b[A\n",
      "36578 examples [00:13, 2796.90 examples/s]\u001b[A\n",
      "36877 examples [00:14, 2849.79 examples/s]\u001b[A\n",
      "37163 examples [00:14, 2821.09 examples/s]\u001b[A\n",
      "37446 examples [00:14, 2754.11 examples/s]\u001b[A\n",
      "37732 examples [00:14, 2784.30 examples/s]\u001b[A\n",
      "38022 examples [00:14, 2817.77 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38306 examples [00:14, 2822.44 examples/s]\u001b[A\n",
      "38613 examples [00:14, 2891.35 examples/s]\u001b[A\n",
      "38909 examples [00:14, 2908.45 examples/s]\u001b[A\n",
      "39201 examples [00:14, 2911.31 examples/s]\u001b[A\n",
      "39517 examples [00:14, 2978.84 examples/s]\u001b[A\n",
      "39834 examples [00:15, 3032.03 examples/s]\u001b[A\n",
      "40138 examples [00:15, 2957.30 examples/s]\u001b[A\n",
      "40435 examples [00:15, 2913.94 examples/s]\u001b[A\n",
      "40728 examples [00:15, 2821.67 examples/s]\u001b[A\n",
      "41012 examples [00:15, 2683.67 examples/s]\u001b[A\n",
      "41283 examples [00:15, 2632.12 examples/s]\u001b[A\n",
      "41548 examples [00:15, 2572.40 examples/s]\u001b[A\n",
      "41836 examples [00:15, 2655.74 examples/s]\u001b[A\n",
      "42116 examples [00:15, 2695.35 examples/s]\u001b[A\n",
      "42394 examples [00:16, 2717.93 examples/s]\u001b[A\n",
      "42679 examples [00:16, 2752.96 examples/s]\u001b[A\n",
      "42956 examples [00:16, 2737.21 examples/s]\u001b[A\n",
      "43255 examples [00:16, 2808.35 examples/s]\u001b[A\n",
      "43544 examples [00:16, 2832.24 examples/s]\u001b[A\n",
      "43840 examples [00:16, 2868.14 examples/s]\u001b[A\n",
      "44128 examples [00:16, 2815.69 examples/s]\u001b[A\n",
      "44411 examples [00:16, 2730.56 examples/s]\u001b[A\n",
      "44698 examples [00:16, 2770.42 examples/s]\u001b[A\n",
      "44989 examples [00:16, 2810.76 examples/s]\u001b[A\n",
      "45271 examples [00:17, 2810.52 examples/s]\u001b[A\n",
      "45571 examples [00:17, 2861.89 examples/s]\u001b[A\n",
      "45858 examples [00:17, 2809.82 examples/s]\u001b[A\n",
      "46140 examples [00:17, 2722.16 examples/s]\u001b[A\n",
      "46443 examples [00:17, 2806.27 examples/s]\u001b[A\n",
      "46726 examples [00:17, 2780.74 examples/s]\u001b[A\n",
      "47010 examples [00:17, 2797.15 examples/s]\u001b[A\n",
      "47297 examples [00:17, 2814.35 examples/s]\u001b[A\n",
      "47579 examples [00:17, 2771.78 examples/s]\u001b[A\n",
      "47857 examples [00:17, 2745.86 examples/s]\u001b[A\n",
      "48151 examples [00:18, 2798.88 examples/s]\u001b[A\n",
      "48432 examples [00:18, 2729.96 examples/s]\u001b[A\n",
      "48706 examples [00:18, 2685.13 examples/s]\u001b[A\n",
      "48976 examples [00:18, 2660.81 examples/s]\u001b[A\n",
      "49243 examples [00:18, 2637.48 examples/s]\u001b[A\n",
      "49527 examples [00:18, 2694.71 examples/s]\u001b[A\n",
      "49804 examples [00:18, 2715.43 examples/s]\u001b[A\n",
      "50077 examples [00:18, 2670.76 examples/s]\u001b[A\n",
      "50356 examples [00:18, 2703.43 examples/s]\u001b[A\n",
      "50627 examples [00:18, 2656.55 examples/s]\u001b[A\n",
      "50894 examples [00:19, 2587.07 examples/s]\u001b[A\n",
      "51164 examples [00:19, 2619.81 examples/s]\u001b[A\n",
      "51427 examples [00:19, 2563.18 examples/s]\u001b[A\n",
      "51685 examples [00:19, 2484.78 examples/s]\u001b[A\n",
      "51935 examples [00:19, 2479.67 examples/s]\u001b[A\n",
      "52184 examples [00:19, 2396.74 examples/s]\u001b[A\n",
      "52437 examples [00:19, 2434.49 examples/s]\u001b[A\n",
      "52682 examples [00:19, 2343.70 examples/s]\u001b[A\n",
      "52918 examples [00:19, 2261.21 examples/s]\u001b[A\n",
      "53189 examples [00:20, 2379.10 examples/s]\u001b[A\n",
      "53438 examples [00:20, 2409.22 examples/s]\u001b[A\n",
      "53681 examples [00:20, 2414.56 examples/s]\u001b[A\n",
      "53924 examples [00:20, 2411.76 examples/s]\u001b[A\n",
      "54167 examples [00:20, 2390.66 examples/s]\u001b[A\n",
      "54428 examples [00:20, 2450.20 examples/s]\u001b[A\n",
      "54674 examples [00:20, 2408.73 examples/s]\u001b[A\n",
      "54925 examples [00:20, 2435.36 examples/s]\u001b[A\n",
      "55173 examples [00:20, 2447.02 examples/s]\u001b[A\n",
      "55432 examples [00:20, 2485.09 examples/s]\u001b[A\n",
      "55703 examples [00:21, 2546.30 examples/s]\u001b[A\n",
      "55995 examples [00:21, 2646.25 examples/s]\u001b[A\n",
      "56262 examples [00:21, 2591.34 examples/s]\u001b[A\n",
      "56527 examples [00:21, 2608.38 examples/s]\u001b[A\n",
      "56789 examples [00:21, 2581.39 examples/s]\u001b[A\n",
      "57050 examples [00:21, 2588.14 examples/s]\u001b[A\n",
      "57310 examples [00:21, 2525.14 examples/s]\u001b[A\n",
      "57579 examples [00:21, 2572.13 examples/s]\u001b[A\n",
      "57851 examples [00:21, 2614.05 examples/s]\u001b[A\n",
      "58114 examples [00:22, 2594.24 examples/s]\u001b[A\n",
      "58382 examples [00:22, 2616.81 examples/s]\u001b[A\n",
      "58645 examples [00:22, 2479.50 examples/s]\u001b[A\n",
      "58948 examples [00:22, 2621.60 examples/s]\u001b[A\n",
      "59214 examples [00:22, 2624.15 examples/s]\u001b[A\n",
      "59479 examples [00:22, 2546.06 examples/s]\u001b[A\n",
      "59744 examples [00:22, 2575.42 examples/s]\u001b[A\n",
      "Computing statistics...: 100%|██████████| 2/2 [00:28<00:00, 10.72s/ split]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_dir = '/tmp/tfds'\n",
    "\n",
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.dataset_as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "train_images = np.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "test_images = np.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 784) (60000, 10)\n",
      "Test: (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', train_images.shape, train_labels.shape)\n",
    "print('Test:', test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxPd6Qw3Z98v"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2DnZo3iYj18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 5.25 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 1 in 3.95 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 2 in 3.96 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 3 in 4.14 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 4 in 3.97 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 5 in 3.96 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 6 in 4.06 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 7 in 3.95 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 8 in 4.08 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n",
      "Epoch 9 in 3.95 sec\n",
      "Training set accuracy 0.0987166687846\n",
      "Test set accuracy 0.097999997437\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_train_batches():\n",
    "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
    "  # You can build up an arbitrary tf.data input pipeline\n",
    "  ds = ds.batch(128).prefetch(1)\n",
    "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "  return tfds.dataset_as_numpy(ds)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in get_train_batches():\n",
    "    x = np.reshape(x, (len(x), num_pixels))\n",
    "    y = one_hot(y, num_labels)\n",
    "    params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xC1CMcVNYwxm"
   },
   "source": [
    "We've now used the whole of the JAX API: `grad` for derivatives, `jit` for speedups and `vmap` for auto-vectorization.\n",
    "We used NumPy to specify all of our computation, and borrowed the great data loaders from `tensorflow/datasets`, and ran the whole thing on the GPU."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-network-and-data-loading.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
